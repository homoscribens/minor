# Hazumi1902


## 実験用ダンプファイルの概要:
対話中のユーザ（実験参加者）の心象を含むラベルデータを予測するためのマルチモーダル機械学習実験を行うために利用可能です．このファイルは， 機械学習の入力として，ユーザの音声，映像，音声認識の結果から得られる発話内容（言語）から抽出したマルチモーダル特徴量（実数値），機械学習の出力として興味度・心象・話題継続アノテーションの値（連続値とカテゴリ値）を格納しています．


### フォルダの構造とファイル：
1902Data  
|---------	1902F2001.csv  
|---------	1902F2010.csv  
:  
:  

各ファイル（1902F2001.csv）は各セッションにおける参加者の発話対ごとの
マルチモーダル特徴量とアノテーションの値を含んでいます．


### 実験用ダンプファイル中のデータ説明：
Hazumi1712のデータセット：
各列はデータの属性，各行は1発話対で抽出された各特徴量の数値を格納しています．
1行目はデータの属性名を示しています．


#### start(agent), start(user), end(user):
ユーザの上半身動画（mp4）におけるシステム, ユーザの発話開始, 終了時間 

#### kinectstart(agent), kinectstart(user), kinectend(user): 
kinectデータにおけるシステム, ユーザの発話開始, 終了時間 (kinect)

#### SS_ternary , TC_ternary, TS_ternary : 
self-sentiment（ユーザが付与した自身の心象ラベル）, interest（興味度ラベル）, topic continuance（話題継続ラベル）のアノテーションの値(IN,TC,TS)を，三段階でラベル化したもの. 2が高群, 1がニュートラル, 0が低群. 

##### 3段階のラベル化方法：
self-sentimentの場合，5点以上を高群，4点をニュートラル，3点以下を低群と決定
Interestの場合: (o)を1, (t)を0, (x)を-1としてアノテーションの平均値を計算して1より
高ければ高群, -1より低ければ低群, それ以外は中間群と決定. 
Topic continuanceとThird sentimentの場合アノテータの平均値が4.5より高ければ高群, 3.5より低ければ低群, それ以外は中間群と決定. 

#### SS：
ユーザ本人により付与されたself-sentimentのアノテーション値. 

#### TC1~TC5:
5名により付与されたtopic continuanceのアノテーション値.

#### TS1~TS5:
5名により付与されたthird sentimentのアノテーション値．

#### pcm_RMSenergy_sma_max ~ F0_sma_de_kurtosis: 
OpenSmileによって抽出された韻律特徴量. Config fileはIS09_emotion.conf.

#### word#001~su： 
ユーザとシステムの発話から抽出された言語特徴量.

#### 17_acceleration_max~AU45_c_mean: 
顔表情特徴量. 「17」のような数字はopenfaceのlandmarkを示す. AUはaction unit.
landmark特徴量はOpenfaceから得られる2次元座標データ(30fps)をもとに目の周り, 口の周りなどの10点のフレーム間速度, 加速度をexchange区間ごとに求め最大値, 平均値, 標準偏差を特徴量とする. またexchange区間のAUの有無の平均を特徴量とする. 

#### HandLeft_acceleration_max~duration: 
動作特徴量. Durationは発話時間
Kinectの3次元間座標データ(30fps)をもとに頭部, 肩などの部位のフレーム間速度, 加速度を求め, exchange区間ごとに最大値, 最小値, 平均値を特徴量とする. 


### マルチモーダルデータの同期方法：
ユーザの音声データ・姿勢データはKinectV2センサより，RGB画像データはビデオカメラより取得した．
KinectV2のデータ記録時間（フレームタイミング）とビデオカメラのデータ記録時間において，各セッションのエージェント発話開始点を特定して遅延を計算し，両デバイスから得られたデータの時刻同期を行った．またMMDエージェントの発話ログファイルを用いて，
ユーザの発話区間・エージェントの発話区間の切り分けを行った．
言語特徴量（素性）は発話区間に対応する音声データに対して音声認識を行うことでテキスト変換した後に抽出した．上記の手順で，音声・言語・画像の情報の時間同期を行った．
